{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project - Machin Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoriyaBitton/Machine_learning/blob/main/Final_Project_Machin_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<u>Final Project - Machin Learning</u>**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "beUyB8ixhH0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Moriya Bitton** &emsp; **ID: 316451749** \n",
        "<br>**Hay Hoffman** &emsp; **ID: 203265186**"
      ],
      "metadata": {
        "id": "8RfcWcUeAeLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<u>Introduction</u>**\n",
        "\n",
        "Machine learning (ML) algorithms are a group of algorithms with a common feature of linearity, which perform regression and classification tasks, usually for high-dimensional data, by finding the rule they represent. \n",
        "<br>In this algorithm, a line or hyperplane is used to separate the data into different labels (for classification) or to construct an equation that predicts the future values of a given point in the data (for regression), based on other points' values. Unlike high-dimensional data, images have a grid structure and are defined by their connections between pixels, which makes conventional ML algorithms unsuitable for image labeling. \n",
        "\n",
        "Deep neural networks (DNN) are nonlinear algorithms that use convolution neural networks (CNN) to overcome ML's obstacles. In CNN's, pixels within an image are learned spatially by multiplying learnable matrices along the image called kernels, reducing their size, and activating nonlinear functions. \n",
        "\n",
        "The extraction of features from data is an important step in any learning process, whether it is deep learning (DL) or machine learning (ML). In most algorithms, it is part of their operation, but for others, it is their primary function. Autoencoders and principal component analysis (PCA) are two examples of these algorithms.\n",
        "\n",
        "By reducing the dataset dimension, PCA improves interpretability and minimizes information loss. An eigenvalue/eigenvector problem is solved by PCA, resulting in the creation of new variables that maximize dataset variance.\n",
        "\n",
        "An autoencoder reduces the dimensionality of data by focusing on relevant areas. In this process, input data is compressed, encoded, and then reconstructed as an output to reduce noise. In an autoencoder, incoming data is systematically reduced in complexity by multiple layers inside a neural network, resulting in a dramatically compressed version of the original. The decoding stage reconstructs the encoded data, with the output representing the input equivalently.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "h1o-CPONXPme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<u>Project's objectives:</u>**\n",
        "\n",
        "We aim to answer two questions:\n",
        "\n",
        "1. Are traditional machine learning algorithms capable of classifying images despite their limitations?\n",
        "\n",
        "2. How do feature extraction algorithms perform better as preprocessors?\n",
        "\n",
        "<br>In our first experiment, we will only reshape and normalize images. <br>Next, we will feed the extracted data into the algorithm to compress it. \n",
        "\n",
        "<br>To test our research question, we will use the most common machine learning algorithms and compare their results with a simple deep learning network.\n",
        "<br>We will train and test using the handwritten Mnisnt digits, which contain black and white 28x28 images with more than 70,000 images.\n",
        "<br>Our DL models are built using TensorFlow and open-source libraries like sklearn.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b78r4PuGG7Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Impotrs**\n"
      ],
      "metadata": {
        "id": "0b6avHiCdRXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic \n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset \n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Model \n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier  \n",
        "\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras.models import Model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "hehSNGZavdKe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **MNIST - DataSet**\n",
        "\n",
        "[Wikipedia page](https://en.wikipedia.org/wiki/MNIST_database)\n"
      ],
      "metadata": {
        "id": "LxjDp80seC52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "rLyKTsTBhQGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abfaff1-57dd-47dd-d199-417a795604bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>Data reshaping and normalization</u>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQwMLROTiEBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "metadata": {
        "id": "Fm261w2_MWBe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of dataset\n",
        "\n",
        "print('X_train: ' + str(x_train.shape), '\\tY_train: ' + str(y_train.shape))\n",
        "print('X_test:  ' + str(x_test.shape), '\\tY_test:  ' + str(y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59cd425f-43e4-4a3c-abe5-760a7320d3a4",
        "id": "RfsqUH75L1QE"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (60000, 784) \tY_train: (60000,)\n",
            "X_test:  (10000, 784) \tY_test:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotting(history):\n",
        "    colors = {'loss':'r', 'accuracy':'b', 'val_loss':'m', 'val_accuracy':'g'}\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.xlabel(\"Epoch\")\n",
        "\n",
        "    for measure in history.keys():\n",
        "        color = colors[measure]\n",
        "        ln = len(history[measure])\n",
        "        plt.plot(range(1,ln+1), history[measure], color + '-', label=measure)  # use last 2 values to draw line\n",
        "\n",
        "    plt.legend(loc='upper left', scatterpoints = 1, frameon=False)"
      ],
      "metadata": {
        "id": "k7qRImt0Jssg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CBEwlpgsVrvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<u>Experiment 1: DL model vs. classic ML model without data preprocessing </u>**\n",
        "<br>\n",
        "\n",
        "  \n",
        "\n",
        "*   Neural networks<br>\n",
        "    We present a simple, fully-connected neural network constructed from a densely-connected NN layer with a relu activation function, followed by a dropout layer and a softmax layer at the output. We use the cross-entropy loss, which is the most common loss for classification problems, with adam  as an optimizer. We train the model with 128 batch sizes and 100 epochs.\n",
        "    <br>\n",
        "    \n",
        "*   Logistic Regression <br>\n",
        "    In statistical analysis, logistic regression predicts whether a data set    belongs to a predefined class based on prior observations.\n",
        "    By using the sigmoid function, logistic regression can set model output from 0 to 1, and the logistic loss is applied to penalize data classification errors.\n",
        "    <br>\n",
        "\n",
        "*   KNN <br>\n",
        "    A k-nearest neighbor algorithm solves classification and regression problems by assuming that similar things exist nearby.\n",
        "    Using a distance function, the algorithm classifies a new sample with the label of the k nearest neighbors whose distance is the smallest from the dataset.\n",
        "    <br>\n",
        "\n",
        "*   SVM <br>\n",
        "    The Support Vector Machine finds the hyperplane that labels the dataset using a linear model.\n",
        "    Based on the constant margin from both sides of the hyperplane, the algorithm reduces the plane weights to find the closest point, creating the support vectors.\n",
        "    Rather than learning the data itself, SVM learns the rules that the data represent instead of the data itself.\n",
        "    <br>\n",
        "\n",
        "*   Random Forest <br>\n",
        "    A random forest is a supervised machine learning algorithm that is constructed from decision tree algorithms. This algorithm is applied in various industries such as banking and e-commerce to predict behavior and outcomes.\n",
        "    <br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oWA9hqp9rRk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = keras.utils.np_utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "DZlkkVN_8x3N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Neural Network Implementation**\n"
      ],
      "metadata": {
        "id": "EYFX0Prp_-bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J17Y6xg1KNBu",
        "outputId": "f0e07f31-7933-4c1f-c6ef-5bded222ac16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Models Comparison**"
      ],
      "metadata": {
        "id": "3uFa_dGU0r6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN - training and evaluation\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=20,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Logistic reggression - Training and evaluation \n",
        "clf_lr_org = LogisticRegression()\n",
        "clf_lr_org.fit(x_train, np.argmax(y_train, axis=1))\n",
        "acc_lr_org = clf_lr_org.score(x_test, np.argmax(y_test, axis=1))\n",
        "\n",
        "# SVM - Training and evaluation \n",
        "clf_svm_org = LinearSVC()\n",
        "clf_svm_org.fit(x_train, np.argmax(y_train, axis=1))\n",
        "pred_svm_org = clf_svm_org.predict(x_test)\n",
        "acc_svm_org = accuracy_score(np.argmax(y_test, axis=1), pred_svm_org)\n",
        "\n",
        "# SVM with RBF kernel - Training and evaluation \n",
        "clf_svm_RBF_org = SVC(kernel='rbf', probability=True, C=1, gamma=0.1)\n",
        "clf_svm_RBF_org.fit(x_train, np.argmax(y_train, axis=1))\n",
        "pred_svm_RBF_org = clf_svm_RBF_org.predict(x_test)\n",
        "acc_svm_RBF_org = accuracy_score(np.argmax(y_test, axis=1), pred_svm_RBF_org) \n",
        "\n",
        "# KNN - Training and evaluation \n",
        "clf_knn_org = KNeighborsClassifier()\n",
        "clf_knn_org.fit(x_train, np.argmax(y_train, axis=1))\n",
        "pred_knn_org = clf_knn_org.predict(x_test)\n",
        "acc_knn_org = accuracy_score(np.argmax(y_test, axis=1), pred_knn_org)\n",
        "\n",
        "# Random Forest - Training and evaluation \n",
        "clf_rf_org = RandomForestClassifier(n_estimators=120)\n",
        "clf_rf_org.fit(x_train,np.argmax(y_train, axis=1))\n",
        "pred_rf_org = clf_rf_org.predict(x_test)\n",
        "acc_rf_org = accuracy_score(np.argmax(y_test, axis=1), pred_rf_org)"
      ],
      "metadata": {
        "id": "DCygZJZFdflh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f8cfa2-0eea-489a-d426-7b5a25af5725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 14s 28ms/step - loss: 0.2458 - accuracy: 0.9259 - val_loss: 0.1065 - val_accuracy: 0.9654\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 14s 29ms/step - loss: 0.1000 - accuracy: 0.9692 - val_loss: 0.0825 - val_accuracy: 0.9745\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0734 - accuracy: 0.9767 - val_loss: 0.0726 - val_accuracy: 0.9770\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0561 - accuracy: 0.9822 - val_loss: 0.0638 - val_accuracy: 0.9818\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0461 - accuracy: 0.9849 - val_loss: 0.0693 - val_accuracy: 0.9798\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0406 - accuracy: 0.9869 - val_loss: 0.0652 - val_accuracy: 0.9802\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0326 - accuracy: 0.9890 - val_loss: 0.0705 - val_accuracy: 0.9793\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.0714 - val_accuracy: 0.9817\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0273 - accuracy: 0.9910 - val_loss: 0.0659 - val_accuracy: 0.9804\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0267 - accuracy: 0.9912 - val_loss: 0.0709 - val_accuracy: 0.9824\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0254 - accuracy: 0.9919 - val_loss: 0.0644 - val_accuracy: 0.9833\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.0650 - val_accuracy: 0.9827\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.0619 - val_accuracy: 0.9853\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.0832 - val_accuracy: 0.9810\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0191 - accuracy: 0.9937 - val_loss: 0.0760 - val_accuracy: 0.9833\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.0808 - val_accuracy: 0.9821\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.0708 - val_accuracy: 0.9848\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0183 - accuracy: 0.9941 - val_loss: 0.0686 - val_accuracy: 0.9832\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0714 - val_accuracy: 0.9831\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0734 - val_accuracy: 0.9846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Results**"
      ],
      "metadata": {
        "id": "_IDyWOrULNRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test results:\\n\")\n",
        "print(100*'-')\n",
        "\n",
        "print(\"\\nDeep neural network accuracy:\\t{:.2f}%\".format(score[1]*100))\n",
        "print(\"\\nLogisticl regression accuracy:\\t{:.2f}%\".format(acc_lr_org*100))\n",
        "print(\"\\nKNN accuracy:\\t{:.2f}%\".format(acc_knn_org*100))\n",
        "print(\"\\nSVM accuracy:\\t{:.2f}%\".format(acc_svm_org*100))\n",
        "print(\"\\nSVM with RBF kernel accuracy:\\t{:.2f}%\".format(acc_svm_RBF_org*100))\n",
        "print(\"\\nRandom Forest accuracy:\\t{:.2f}%\".format(acc_rf_org*100))"
      ],
      "metadata": {
        "id": "ANXgwmHj-_r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Conclusions and Observations** \n",
        "We see from the results that DNN performs better than classic ML algorithms. We conjectured that the ML algorithms would not be able to accomplish the task of classifying images correctly, but the results show that we were wrong. This result raises another question, for what complexity level of images, such as classifying RGB images, or more extensive datasets such as ImageNet that contains 1000 classes, classic ML algorithms will provide suitable solutions, as observed here. \n",
        "\n",
        "Maybe a better research question should have been: \"what is the complexity threshold that ML algorithms still manage to result well\n",
        "?\". \n",
        "\n",
        "Mabey, the answer to this question will open a window for a better understanding of the relation between DL algorithms and ML Algorithms for image classification. "
      ],
      "metadata": {
        "id": "meLnm1mCuUPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest - Note:\n",
        "\n",
        "* n_estimators=120: acc: 97.11%\n",
        "* n_estimators=100: acc: 97.07%\n",
        "* n_estimators=90: acc: 96.96%\n",
        "* n_estimators=50: acc: 96.72%\n",
        "* n_estimators=20: acc: 95.97%\n",
        "\n",
        "Even after increasing the value for n_estimators, the accuracy stays in the range of 96% - 97%."
      ],
      "metadata": {
        "id": "hV85IfJmTYz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "J8onIHvlWDTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<u>Experiment 2: DL model vs. classic ML model with data preprocessing </u>**"
      ],
      "metadata": {
        "id": "U1pxYPBru5Lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Autoencoder Model**\n",
        "\n",
        "As for the encoder, we present a simple, fully-connected neural network constructed from a densely-connected NN layer with a relu activation function and a softmax layer at the output. We use the cross-entropy loss as well, with adam as an optimizer. We train the model with 128 batch sizes and 100 epochs. "
      ],
      "metadata": {
        "id": "OPb7bNReAbbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_dim = 32  \n",
        "\n",
        "input_img = tf.keras.Input(shape=(784,))\n",
        "encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_img)\n",
        "decoded = tf.keras.layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = tf.keras.Model(input_img, decoded)\n",
        "\n",
        "# Encoder\n",
        "encoder = tf.keras.Model(input_img, encoded)\n",
        "\n",
        "# Decoder\n",
        "encoded_input = tf.keras.Input(shape=(encoding_dim,))\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "decoder = tf.keras.Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "xFY6OjsBL68q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(x_train, x_train,\n",
        "                          epochs=20,\n",
        "                          batch_size=128,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "40qX-pBRd4xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotting(history.history)"
      ],
      "metadata": {
        "id": "Q2nEqJC1-5Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs_train = encoder.predict(x_train)\n",
        "encoded_imgs_test = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs_test)"
      ],
      "metadata": {
        "id": "_0ijH1fwDJzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **PCA - Principal Component Analysis**"
      ],
      "metadata": {
        "id": "YiaicK3T13NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(32)\n",
        "pca.fit(x_train)\n",
        "pca_train = pca.transform(x_train)\n",
        "pca_test = pca.transform(x_test)\n",
        "approximation = pca.inverse_transform(pca_test)"
      ],
      "metadata": {
        "id": "2gI2kD9M1zrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **<u>PCA vs. Autoencoder Output</u>**"
      ],
      "metadata": {
        "id": "as1706cqL0aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10  \n",
        "\n",
        "print(\"\\nOriginal Data:\")\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nEncoded Data:\")\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(n):    \n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPCA Data:\")\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(approximation[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cLGOM4eQJWZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Neural Network - Encoded Data**\n",
        "We perform an adaptation for the previously proposed neural network to be compatible with autoencoder output"
      ],
      "metadata": {
        "id": "fpjclPa9AiTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs_train_normalized = encoded_imgs_train / np.max(encoded_imgs_train)\n",
        "print(encoded_imgs_train_normalized[0].shape)\n",
        "encoded_imgs_test_normalized = encoded_imgs_test / np.max(encoded_imgs_test)\n",
        "\n",
        "encoded_model = Sequential()\n",
        "encoded_model.add(Dense(512, activation='relu', input_shape=(encoding_dim,)))\n",
        "encoded_model.add(Dropout(0.2))\n",
        "encoded_model.add(Dense(512, activation='relu'))\n",
        "encoded_model.add(Dropout(0.2))\n",
        "encoded_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "encoded_model.summary()\n",
        "\n",
        "encoded_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "VbcJAvk7d9Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Neural Network - PCA Data**"
      ],
      "metadata": {
        "id": "G3xxWI0UAORF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_model = Sequential()\n",
        "pca_model.add(Dense(512, activation='relu', input_shape=(32,)))\n",
        "pca_model.add(Dropout(0.2))\n",
        "pca_model.add(Dense(512, activation='relu'))\n",
        "pca_model.add(Dropout(0.2))\n",
        "pca_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "pca_model.summary()\n",
        "\n",
        "pca_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Q8s_DFxrAaQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Learning Models Comparison**"
      ],
      "metadata": {
        "id": "VML4u8CKK_AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN - Training and evaluation\n",
        "\n",
        "# ENCODED DATA\n",
        "encoded_history = encoded_model.fit(encoded_imgs_train_normalized, y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=20,\n",
        "                    verbose=1,\n",
        "                    validation_data=(encoded_imgs_test_normalized, y_test))\n",
        "\n",
        "encoded_score = encoded_model.evaluate(encoded_imgs_test_normalized, y_test, verbose=0)\n",
        "\n",
        "# PCA DATA\n",
        "PCA_history = pca_model.fit(pca_train, y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=20,\n",
        "                    verbose=1,\n",
        "                    validation_data=(pca_test, y_test))\n",
        "pca_score = pca_model.evaluate(pca_test, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "YEJLjIpjHRLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic reggression - Training and evaluation \n",
        "\n",
        "# ENCODED DATA\n",
        "clf_lr_enc = LogisticRegression()\n",
        "clf_lr_enc.fit(encoded_imgs_train_normalized, np.argmax(y_train, axis=1))\n",
        "acc_lr_enc = clf_lr_enc.score(encoded_imgs_test_normalized, np.argmax(y_test, axis=1))\n",
        "\n",
        "# PCA DATA\n",
        "clf_lr_pca = LogisticRegression()\n",
        "clf_lr_pca.fit(pca_train, np.argmax(y_train, axis=1))\n",
        "acc_lr_pca = clf_lr_pca.score(pca_test, np.argmax(y_test, axis=1))"
      ],
      "metadata": {
        "id": "vtmuJrhQVj1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN - Training and evaluation \n",
        "\n",
        "# ENCODED DATA\n",
        "clf_knn_enc = KNeighborsClassifier()\n",
        "clf_knn_enc.fit(encoded_imgs_train_normalized, np.argmax(y_train, axis=1))\n",
        "pred_knn_enc = clf_knn_enc.predict(encoded_imgs_test_normalized)\n",
        "acc_knn_enc = accuracy_score(np.argmax(y_test, axis=1), pred_knn_enc)\n",
        "\n",
        "# PCA DATA\n",
        "clf_knn_pca = KNeighborsClassifier()\n",
        "clf_knn_pca.fit(pca_train, np.argmax(y_train, axis=1))\n",
        "pred_knn_pca = clf_knn_pca.predict(pca_test)\n",
        "acc_knn_pca = accuracy_score(np.argmax(y_test, axis=1), pred_knn_pca)\n"
      ],
      "metadata": {
        "id": "qvWn3F78Vj-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM - Training and evaluation \n",
        "\n",
        "# ENCODED DATA\n",
        "clf_svm_enc = LinearSVC()\n",
        "clf_svm_enc.fit(encoded_imgs_train_normalized, np.argmax(y_train, axis=1))\n",
        "pred_svm_enc = clf_svm_enc.predict(encoded_imgs_test_normalized)\n",
        "acc_svm_enc = accuracy_score(np.argmax(y_test, axis=1), pred_svm_enc)\n",
        "\n",
        "# PCA DATA\n",
        "clf_svm_pca = LinearSVC()\n",
        "clf_svm_pca.fit(pca_train, np.argmax(y_train, axis=1))\n",
        "pred_svm_pca = clf_svm_pca.predict(pca_test)\n",
        "acc_svm_pca = accuracy_score(np.argmax(y_test, axis=1), pred_svm_pca)"
      ],
      "metadata": {
        "id": "GvFEu2E1kqWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with RBF kernel - Training and evaluation \n",
        "\n",
        "# ENCODED DATA\n",
        "clf_svc_enc = SVC(kernel='rbf', probability=True, C=1, gamma=0.1)\n",
        "clf_svc_enc.fit(encoded_imgs_train_normalized, np.argmax(y_train, axis=1))\n",
        "pred_svc_enc = clf_svc_enc.predict(encoded_imgs_test_normalized)\n",
        "acc_svc_enc = accuracy_score(np.argmax(y_test, axis=1), pred_svc_enc)\n",
        "\n",
        "# PCA DATA\n",
        "clf_svc_pca = SVC(kernel='rbf', probability=True, C=1, gamma=0.1)\n",
        "clf_svc_pca.fit(pca_train, np.argmax(y_train, axis=1))\n",
        "pred_svc_pca = clf_svc_pca.predict(pca_test)\n",
        "acc_svc_pca = accuracy_score(np.argmax(y_test, axis=1), pred_svc_pca)"
      ],
      "metadata": {
        "id": "mFJx441TlhPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest - Training and evaluation \n",
        "\n",
        "# ENCODED DATA\n",
        "clf_rf_enc = RandomForestClassifier(n_estimators=100)\n",
        "clf_rf_enc.fit(encoded_imgs_train_normalized, np.argmax(y_train, axis=1))\n",
        "pred_rf_enc = clf_rf_enc.predict(encoded_imgs_test_normalized)\n",
        "acc_rf_enc = accuracy_score(np.argmax(y_test, axis=1), pred_rf_enc)\n",
        "\n",
        "# PCA DATA\n",
        "clf_rf_pca = RandomForestClassifier(n_estimators=100)\n",
        "clf_rf_pca.fit(pca_train, np.argmax(y_train, axis=1))\n",
        "pred_rf_pca = clf_rf_pca.predict(pca_test)\n",
        "acc_rf_pca = accuracy_score(np.argmax(y_test, axis=1), pred_rf_pca)"
      ],
      "metadata": {
        "id": "QhKNrSAcR-z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Results**"
      ],
      "metadata": {
        "id": "sIUA3bCCLGBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTest Results\\n\")\n",
        "print(100*'-')\n",
        "\n",
        "print(\"\\nDeep Neural Network Accuracy\\n\",\n",
        "      \"\\n\\tEncoded Data:\\t{:.2f}%\\n\".format(encoded_score[1]*100),\n",
        "      \"\\n\\tPCA Data:\\t{:.2f}%\\n\".format(pca_score[1]*100))\n",
        "print(100*'-')\n",
        "\n",
        "print(\"\\nLogisticl Regression Accuracy\\n\",\n",
        "      \"\\n\\tEncoded Data:\\t{:.2f}%\\n\".format(acc_lr_enc*100),\n",
        "      \"\\n\\tPCA Data:\\t{:.2f}%\\n\".format(acc_lr_pca*100))\n",
        "print(100*'-')\n",
        "\n",
        "print(\"\\nKNN Accuracy\\n\",\n",
        "      \"\\n\\tEncoded Data:\\t{:.2f}%\\n\".format(acc_knn_enc*100),\n",
        "      \"\\n\\tPCA Data:\\t{:.2f}%\\n\".format(acc_knn_pca*100))\n",
        "print(100*'-')\n",
        "\n",
        "print(\"\\nSVM Accuracy\\n\",\n",
        "      \"\\n\\tEncoded Data:\\t{:.2f}%\\n\".format(acc_svm_enc*100),\n",
        "      \"\\n\\tPCA Data:\\t{:.2f}%\\n\".format(acc_svm_pca*100))\n",
        "print(100*'-')\n",
        "\n",
        "print(\"\\nSVC Accuracy\\n\",\n",
        "      \"\\n\\tEncoded Data:\\t{:.2f}%\\n\".format(acc_svc_enc*100),\n",
        "      \"\\n\\tPCA Data:\\t{:.2f}%\\n\".format(acc_svc_pca*100))\n",
        "print(100*'-')\n",
        "\n",
        "print(\"\\nRandom Forest Accuracy\\n\",\n",
        "      \"\\n\\tEncoded Data:\\t{:.2f}%\\n\".format(acc_rf_enc*100),\n",
        "      \"\\n\\tPCA Data:\\t{:.2f}%\\n\".format(acc_rf_pca*100))\n",
        "print(100*'-')"
      ],
      "metadata": {
        "id": "e1Z5Cc26DwwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Conclusions and Observations** \n",
        "In this study, DNN performed better than classic ML algorithms once again. \n",
        "\n",
        "The feature extraction, in contrast to intuition, reduced the algorithm's performance, particularly the ML algorithms, which is curious and requires follow-up research that can focus on changing the PCA number of variables and using different autoencoder architectures. \n",
        "\n",
        "All algorithms performed better on the PCA's reduced data than on the autoencoder data; this may be because the PCA was limited to only 32 variables that captured the essence of the data, as demonstrated in the comparison part. Another explanation for the results could be that the autoencoder produces more features than the PCA, which leads to either a) overfitting or b) error during training. \n",
        "\n",
        "Results indicate that the accuracy of different algorithms is about 1%, which doesn't necessarily mean that one is better than another. \n",
        "\n",
        "Future studies could compare the accuracy and PCA number components, as well as different autoencoder architectures."
      ],
      "metadata": {
        "id": "FqJD-11y5pjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest - Note:\n",
        "\n",
        "* PCA still give a better result then Autoencider.\n",
        "\n",
        "* We got a better result without data preprocessing\n",
        "\n",
        "* n_estimators=50: <br>\n",
        "Encoded Data:\t93.93% <br>\n",
        "PCA Data:\t95.09%\n",
        "\n",
        "* n_estimators=90: <br>\n",
        "Encoded Data:\t94.21% <br>\n",
        "PCA Data:\t95.48%\n",
        "\n",
        "* n_estimators=100:\t<br>\n",
        "Encoded Data:\t94.24% <br>\n",
        "PCA Data:\t95.35%\n",
        "\n",
        "* n_estimators=120: <br>\n",
        "Encoded Data:\t94.08% <br>\n",
        "PCA Data:\t95.49%"
      ],
      "metadata": {
        "id": "MGyWOO2qcQmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4ACUtxcKsWFo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}